{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "from typing import Dict, Any\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data.load_and_preprocess import load_raw_data, create_exploded_df, create_sequences, create_vocab\n",
    "from data.datasets import AppSequenceDataset, PreloadedDataset\n",
    "from models.transformer_models import ShallowTransformer, ShallowTransformerWithAttention\n",
    "from training.iterations import train_epoch, evaluate\n",
    "\n",
    "from utils import load_config, safe_load_pickle, safe_save_pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-17 19:13:28.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mLoading and validating cfguration\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:28.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mConverting path strings to Path objects\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:38.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mSequence length: 64\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:38.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mVocabulary size: 634\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Loading and validating cfguration\")\n",
    "cfg_path = Path(\"config/train/default.yaml\")\n",
    "cfg = load_config(cfg_path)\n",
    "\n",
    "logger.info(\"Converting path strings to Path objects\")\n",
    "data_paths = {k: Path(v) for k, v in cfg['data'].items()}\n",
    "\n",
    "# Load raw data if needed\n",
    "if 'all_mappings' in globals():\n",
    "    logger.info(\"Using existing mappings from notebook\")\n",
    "else:\n",
    "    df_day_point, df_app_to_class = load_raw_data(\n",
    "        data_paths['raw_data_path'],\n",
    "        data_paths['app_mappings_path']\n",
    "    )\n",
    "\n",
    "# Try to load cached exploded_df\n",
    "exploded_df = safe_load_pickle(data_paths['exploded_df_path'])\n",
    "if exploded_df is None:\n",
    "    if 'df_day_point' in globals():\n",
    "        logger.info(\"Creating exploded_df from existing df_day_point\")\n",
    "        exploded_df = create_exploded_df(df_day_point)\n",
    "    else:\n",
    "        logger.info(\"Loading raw data and creating exploded_df\")\n",
    "        df_day_point, _ = load_raw_data(\n",
    "            data_paths['raw_data_path'],\n",
    "            data_paths['app_mappings_path']\n",
    "        )\n",
    "        exploded_df = create_exploded_df(df_day_point)\n",
    "    safe_save_pickle(exploded_df, data_paths['exploded_df_path'])\n",
    "\n",
    "# Try to load cached sequences\n",
    "sequences = safe_load_pickle(data_paths['sequences_path'])\n",
    "if sequences is None:\n",
    "    logger.info(\"Creating sequences from exploded_df\")\n",
    "    seq_length = cfg.get('model', {}).get('seq_length', 64)\n",
    "    sequences = create_sequences(exploded_df, seq_length)\n",
    "    safe_save_pickle(sequences, data_paths['sequences_path'])\n",
    "\n",
    "df_sequences = pd.DataFrame(sequences)\n",
    "seq_length = df_sequences['apps'].apply(len).max()\n",
    "logger.info(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "# Try to load cached vocabulary\n",
    "app_to_idx = safe_load_pickle(data_paths['vocab_path'])\n",
    "if app_to_idx is None:\n",
    "    app_to_idx = create_vocab(sequences)\n",
    "    safe_save_pickle(app_to_idx, data_paths['vocab_path'])\n",
    "vocab_size = len(app_to_idx)\n",
    "logger.info(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-17 19:13:38.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mUsing device: cuda\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:38.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mCreating datasets and splitting into train/val sets\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:38.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mDataset for self-supervised training created with mask probability: 0.15\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:39.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCreating data loaders\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(cfg.get('training', {}).get('device') if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "logger.info(\"Creating datasets and splitting into train/val sets\")\n",
    "dataset = AppSequenceDataset(sequences, app_to_idx)\n",
    "\n",
    "dataset = AppSequenceDataset(sequences, app_to_idx, sequence_length=seq_length, mask_prob=cfg.get('training', {}).get('mask_prob', 0.15))\n",
    "logger.info(f\"Dataset for self-supervised training created with mask probability: {cfg.get('training', {}).get('mask_prob', 0.15)}\")\n",
    "\n",
    "train_emp_ids, val_emp_ids = train_test_split(\n",
    "    df_sequences['employeeId'].unique(), \n",
    "    test_size=cfg.get('training', {}).get('test_size', 0.2), \n",
    "    random_state=cfg.get('training', {}).get('random_seed', 42)\n",
    ")\n",
    "\n",
    "train_idx = df_sequences.merge(pd.Series(train_emp_ids, name='employeeId'), on='employeeId', how='inner').index.tolist()\n",
    "val_idx = df_sequences.merge(pd.Series(val_emp_ids, name='employeeId'), on='employeeId', how='inner').index.tolist()\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "if cfg.get('training', {}).get('preload_dataset', False):\n",
    "    logger.info(\"Preloading dataset to device\")\n",
    "    train_dataset = PreloadedDataset(train_dataset, device)\n",
    "    val_dataset = PreloadedDataset(val_dataset, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\"Creating data loaders\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=cfg.get('model', {}).get('batch_size'), \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=cfg.get('model', {}).get('batch_size')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-17 19:13:39.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mInitializing model\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:40.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mShallowTransformerWithAttention(\n",
      "  (app_embeddings): Embedding(635, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (app_predictor): Linear(in_features=64, out_features=635, bias=True)\n",
      ") initialized on device: cuda\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:40.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mInitializing training components\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "logger.info(\"Initializing model\")\n",
    "# model = ShallowTransformerW(\n",
    "#     vocab_size=vocab_size + 1,\n",
    "#     d_model=cfg.get('model', {}).get('d_model', 64),\n",
    "#     nhead=cfg.get('model', {}).get('nhead', 4),\n",
    "#     seq_length=seq_length,\n",
    "# ).to(device)\n",
    "model = ShallowTransformerWithAttention(\n",
    "    vocab_size=vocab_size + 1, # + 1 for MASK\n",
    "    d_model=cfg.get('model', {}).get('d_model', 64),\n",
    "    nhead=cfg.get('model', {}).get('nhead', 4),\n",
    "    seq_length=seq_length,\n",
    "    n_layers=cfg.get('model', {}).get('num_encoder_layers', 3)\n",
    ").to(device)\n",
    "logger.info(f\"{model} initialized on device: {device}\")\n",
    "# Training setup\n",
    "logger.info(\"Initializing training components\")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=float(cfg.get('training', {}).get('lr', 1e-4))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-17 19:13:44.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mTensorBoard logs will be saved to runs\\transformer_64_1e-4\u001b[0m\n",
      "\u001b[32m2024-11-17 19:13:44.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mStarting training loop\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 40.10it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 42.69it/s]\n",
      "\u001b[32m2024-11-17 19:14:32.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 1/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:14:32.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 5.3691\u001b[0m\n",
      "\u001b[32m2024-11-17 19:14:32.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8894\u001b[0m\n",
      "\u001b[32m2024-11-17 19:14:32.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0528\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:37<00:00, 40.84it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 44.38it/s]\n",
      "\u001b[32m2024-11-17 19:15:18.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 2/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:15:18.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 5.0346\u001b[0m\n",
      "\u001b[32m2024-11-17 19:15:18.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8429\u001b[0m\n",
      "\u001b[32m2024-11-17 19:15:18.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0562\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 39.40it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:11<00:00, 36.17it/s]\n",
      "\u001b[32m2024-11-17 19:16:09.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 3/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:09.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 5.0101\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:09.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8271\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:09.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0578\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:37<00:00, 40.39it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 41.49it/s]\n",
      "\u001b[32m2024-11-17 19:16:56.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 4/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:56.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9992\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:56.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8293\u001b[0m\n",
      "\u001b[32m2024-11-17 19:16:56.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0578\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 40.09it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 41.13it/s]\n",
      "\u001b[32m2024-11-17 19:17:45.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 5/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:17:45.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9951\u001b[0m\n",
      "\u001b[32m2024-11-17 19:17:45.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8325\u001b[0m\n",
      "\u001b[32m2024-11-17 19:17:45.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0585\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 39.57it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 41.17it/s]\n",
      "\u001b[32m2024-11-17 19:18:33.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 6/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:18:33.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9936\u001b[0m\n",
      "\u001b[32m2024-11-17 19:18:33.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8195\u001b[0m\n",
      "\u001b[32m2024-11-17 19:18:33.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0588\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:40<00:00, 38.13it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 43.50it/s]\n",
      "\u001b[32m2024-11-17 19:19:23.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 7/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:19:23.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9907\u001b[0m\n",
      "\u001b[32m2024-11-17 19:19:23.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8175\u001b[0m\n",
      "\u001b[32m2024-11-17 19:19:23.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0587\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 39.85it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:09<00:00, 41.53it/s]\n",
      "\u001b[32m2024-11-17 19:20:11.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 8/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:20:11.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9893\u001b[0m\n",
      "\u001b[32m2024-11-17 19:20:11.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8175\u001b[0m\n",
      "\u001b[32m2024-11-17 19:20:11.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0586\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 39.89it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:10<00:00, 37.81it/s]\n",
      "\u001b[32m2024-11-17 19:21:01.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 9/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:01.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9899\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:01.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8244\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:01.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0586\u001b[0m\n",
      "Training: 100%|██████████| 1534/1534 [00:38<00:00, 39.55it/s]\n",
      "Evaluating: 100%|██████████| 404/404 [00:12<00:00, 33.37it/s]\n",
      "\u001b[32m2024-11-17 19:21:52.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch 10/10\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:52.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTrain Loss: 4.9881\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:52.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVal Loss: 4.8163\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:52.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAccuracy: 0.0587\u001b[0m\n",
      "\u001b[32m2024-11-17 19:21:52.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mSaving model checkpoint\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Add TensorBoard writer setup after model initialization\n",
    "log_dir = Path(\"runs\") / f\"transformer_{seq_length}_{cfg['training']['lr']}\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "logger.info(f\"TensorBoard logs will be saved to {log_dir}\")\n",
    "\n",
    "logger.info(\"Starting training loop\")\n",
    "n_epochs = cfg.get('model', {}).get('num_epochs', 10)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/validation', accuracy, epoch)\n",
    "    \n",
    "    # Add learning rate tracking\n",
    "    writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Optional: Log model parameters histograms\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f'Parameters/{name}', param.data, epoch)\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    logger.info(f\"Train Loss: {train_loss:.4f}\")\n",
    "    logger.info(f\"Val Loss: {val_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Close the writer at the end\n",
    "writer.close()\n",
    "\n",
    "if cfg.get('training', {}).get('save_model_path', None):\n",
    "    logger.info(\"Saving model checkpoint\")\n",
    "    model_path = Path(cfg.get('training', {}).get('save_model_path'))\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'app_to_idx': app_to_idx,\n",
    "    }, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
